# Architectural Overview

spcasm processes assembly code in several steps, which can be thought of as roughly equivalent to "passes" in traditional assemblers. The processing can be split up into three main steps: lexing and parsing (1-3), expanding and making the AST concrete (4-8), and assembly (9-11).

1. **Lexing.** A custom lexer with minimal lookahead splits the input text into tokens and already performs some degree of high-level processing, like parsing numbers and test comments (if applicable) as well as distinguishing between directives, mnemonics and references.
2. **Token stream transformation.** This step is necessary as the SPC700 assembly language with complex math expressions is not LR(1) (in fact, as far as I can tell it's context-sensitive), so we cannot pass the plain tokens to the parser. Instead, some transformations with significant lookahead need to happen, for example determining what parenthesis are used for (indexing as an addressing mode or math expressions?), combining "+X" and "+Y" and appending a newline to enable the top-level producer grammar.
3. **LR(1) parsing.** The parser is an auto-generated [LALRPOP](https://github.com/lalrpop/lalrpop) parser that receives the more context-aware modified token stream. The .lalrpop file additionally contains driver code that builds the AST and performs various early decisions and resolutions, like selecting direct page addressing if possible.
4. **Assembly source inclusion.** The `include` directives include the assembly source code from other files, and after the AST is finalized, all requested additional files are resolved. The assembler keeps a memory cache of already-parsed files, so that if you have commonly-included files, the cost of including them more than once is reduced. Handling other files goes through the exact same pipeline as the main file, so it repeats steps 1-5 and possibly 6 if there are included files in the included file. Circular includes are not possible for obvious reasons (though non-circular repeated includes are allowed as there are many valid use cases) and will be detected even before the lexer runs. After an included file's AST is finalized, it is copied into the original AST where it replaces the `include` directive.
5. **User-defined macro resolution.** The assembler detects all user-defined macros and resolves them by replacing them with generated AST. This also means that spcasm's macro functionality is far more sophisticated than simple copy-paste, it can rather be thought of as a function inliner in a traditional compiler. This step includes resolving compile-time programming constructs such as conditional compilation with `if`.
6. **Label resolution.** There are several kinds of labels who need to be resolved later than during parsing, such as relative labels, local labels, and pseudo labels. This is fixed in several independent analysis steps, and afterwards, only proper labels should exist in a well-formed program.
7. **Segmentation.** The assembler splits the program into its physical segments by linearly traversing the fully expanded AST and handling segment control macros, such as `org` or `pushpc`. The result is a collection of segments with known starting addresses and the contained program elements, still in AST form.
8. **Direct page references optimization.** Given the new knowledge of segments, the assembler can infer memory addresses for most kinds of program elements. Therefore, the assembler is able to deduce which references will live in the direct page. This is important as many instructions are smaller and faster when using direct page addressing, so they should be used whenever possible. The assembler uses an iterative optimization process that keeps as many labels as possible in the direct page. For a detailed explanation, see the well-commented `spcasm::parser::AssemblyFile::optimize_direct_page_labels` function. The optimization problem solved here has been compared to linker relaxation for jump offsets and thunks; however while optimal linker relaxation is most likely NP-hard, spcasm (minus some bugs) solves the direct page reference optimization problem, which is in P, optimally.
9. **Partially-symbolic assembly.** The assembler assembles a mix of high and low-level memory data from instructions and directives. Where possible, concrete memory data is provided, but in other cases, symbolic data like "lowest byte of this reference's address" is used instead when necessary. It is important to notice that memory addresses are fixed after this step.
10. **Final symbol resolution.** Because addresses were fixed in the last step, symbolic data can now be resolved to concrete data. This is done in several passes to allow e.g. mention of references before they are defined. The step both involves obtaining concrete values for symbols based on the fixed memory layout, and emplacing concrete data based on symbols that were newly resolved. For example, a reference's value can be resolved to the memory location of the data that is tagged to be the position of the reference. Then, references to the reference can be replaced by the actual memory address.
11. **Output format assembly.** Before this section, the binary data exists in several independent segments. Now, depending on the output format, the data is assembled into its final form and written out. The details of this step of course depend on the format, such that a format like ELF will keep the segments separate while a ROM image will concatenate segments.

In terms of performance, parsing and label resolution have consistently been the most expensive operations, each taking at least 20% of run time. No exact metrics on memory usage exist, since spcasm is a highly memory-efficient program that consumes only a few megabytes peak.
